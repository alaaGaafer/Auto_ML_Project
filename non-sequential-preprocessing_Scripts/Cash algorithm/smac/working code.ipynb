{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install smac","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-06-24T16:00:57.290661Z","iopub.execute_input":"2024-06-24T16:00:57.291273Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Collecting smac\n  Downloading smac-2.1.0.tar.gz (148 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.1/148.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.23.3 in /opt/conda/lib/python3.10/site-packages (from smac) (1.26.4)\nRequirement already satisfied: scipy>=1.9.2 in /opt/conda/lib/python3.10/site-packages (from smac) (1.11.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from smac) (5.9.3)\nCollecting pynisher>=1.0.0 (from smac)\n  Downloading pynisher-1.0.10.tar.gz (30 kB)\n  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hCollecting ConfigSpace>=0.6.1 (from smac)\n  Downloading ConfigSpace-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.2 kB)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from smac) (1.4.2)\nRequirement already satisfied: scikit-learn>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from smac) (1.2.2)\nCollecting pyrfr>=0.9.0 (from smac)\n  Downloading pyrfr-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (601 bytes)\nRequirement already satisfied: dask[distributed] in /opt/conda/lib/python3.10/site-packages (from smac) (2024.5.2)\nCollecting dask-jobqueue (from smac)\n  Downloading dask_jobqueue-0.8.5-py2.py3-none-any.whl.metadata (1.6 kB)\nCollecting emcee>=3.0.0 (from smac)\n  Downloading emcee-3.1.6-py2.py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from smac) (2023.12.25)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from smac) (6.0.1)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from ConfigSpace>=0.6.1->smac) (3.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ConfigSpace>=0.6.1->smac) (4.9.0)\nRequirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from ConfigSpace>=0.6.1->smac) (10.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.1.2->smac) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from ConfigSpace import Configuration, ConfigurationSpace\n\nimport numpy as np\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom sklearn import datasets\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\niris = datasets.load_iris()\n\n\ndef train(config: Configuration, seed: int = 0) -> float:\n    classifier = SVC(C=config[\"C\"], random_state=seed)\n    scores = cross_val_score(classifier, iris.data, iris.target, cv=5)\n#     print (np.mean(scores))\n    return 1 - np.mean(scores)\n\n\nconfigspace = ConfigurationSpace({\"C\": (0.100, 1000.0)})\n\n# Scenario object specifying the optimization environment\nscenario = Scenario(configspace, deterministic=True, n_trials=200)\n\n# Use SMAC to find the best configuration/hyperparameters\nsmac = HyperparameterOptimizationFacade(scenario, train)\nincumbent = smac.optimize()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"incumbent.values\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier = SVC(C=6.724148702862232)\nscores = cross_val_score(classifier, iris.data, iris.target, cv=5)\nprint(np.mean(scores))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV, ShuffleSplit\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier\nimport seaborn as sn\nimport warnings\n\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the Credit Card Fraud Detection dataset\nurl = \"https://www.openml.org/data/get_csv/31/dataset_31.csv\"\ndf = pd.read_csv(url)\n\n# Drop missing values\ndf = df.dropna()\n\n# Convert categorical features to numerical\ndf = pd.get_dummies(df, drop_first=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['class_good'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"target = \"class_good\"\nX = df.drop(columns=target)\ny = df[target]\n\n# Split the dataset into training and testing sets\nX_train_raw, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_selection import SelectKBest, f_classif\n\ndef Which_features(X_train,y_train,number_of_columns):\n\n    # Select the top k features based on ANOVA F-statistic\n    selector = SelectKBest(score_func=f_classif, k=number_of_columns)\n    X_train_selected = selector.fit_transform(X_train, y_train)\n\n    # Get the column names of the selected features\n    selected_feature_names = X_train.columns[selector.get_support()]\n\n    \n    return list(selected_feature_names)\n\nselected_features = Which_features(X_train_raw,y_train,number_of_columns=15)\nselected_features","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X =X[selected_features]\nX_train_raw = X_train_raw[selected_features]\nX_test = X_test[selected_features]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_raw.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifer=KNeighborsClassifier(n_neighbors=7)\nclassifer.fit(X_train_raw,y_train)\ny_pred = classifer.predict(X_test)\nloss=1-accuracy_score(y_test,y_pred)\nprint(loss)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_from_smac(smac) -> None:\n    plt.figure()\n    configs = smac.runhistory.get_configs()\n    incumbents = smac.intensifier.get_incumbents()\n\n    for i, config in enumerate(configs):\n        if config in incumbents:\n            continue\n\n        label = None\n        if i == 0:\n            label = \"Configuration\"\n\n        x = config[\"x\"]\n        f1, f2 = mymodell(x)\n        plt.scatter(f1, f2, c=\"blue\", alpha=0.1, marker=\"o\", zorder=3000, label=label)\n\n    for i, config in enumerate(incumbents):\n        label = None\n        if i == 0:\n            label = \"Incumbent\"\n\n        x = config[\"x\"]\n        f1, f2 = mymodell(x)\n        plt.scatter(f1, f2, c=\"red\", alpha=1, marker=\"x\", zorder=3000, label=label)\n\n    plt.xlabel(\"f1\")\n    plt.ylabel(\"f2\")\n    plt.title(\"Schaffer 2D\")\n    plt.legend()\n\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from ConfigSpace import Categorical, Configuration, ConfigurationSpace, Float, Integer\nfrom ConfigSpace.conditions import InCondition,EqualsCondition\nfrom smac import HyperparameterOptimizationFacade, Scenario\nfrom sklearn.linear_model import Lasso, Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom smac import Callback\nimport time\n#potential optimizations\n#add timeserie\n#search for the best hypers parameters for the each model\n#add random state parameters\n#what is the problem with linear svc\nclass CustomCallback(Callback):\n    def __init__(self):\n        self.trials_counter=0\n    def on_start(self, smbo):\n        print(\"let's start the optimization\")\n    def on_tell_end (self, smbo,info, value):\n        self.trials_counter+=1\n        print(f\"the number of trials is: {self.trials_counter}\")\n        if self.trials_counter%10==0:\n            incumbents = smbo.intensifier.get_incumbents()\n            for incumbent in incumbents:\n                print(f\"the incumbent is: {incumbent.get_dictionary()}\")\n                print(f\"the incumbent loss is: {smbo.runhistory.get_cost(incumbent)}\")\n            if self.trials_counter==100:\n                print(\"let's stop the optimization at trial 100\")\n                return False\n            return None\nclass Models:\n    def __init__(self,similarModels,problemType,X_train,Y_train,X_test,Y_test):\n        self.Models=similarModels\n        self.Problemtype=problemType\n        self.X_train=X_train\n        self.Y_train=Y_train\n        self.X_test=X_test\n        self.Y_test=Y_test\n    def configspace(self):\n        confs = ConfigurationSpace(seed=0)\n        #HPOs\n        if self.Problemtype=='Classification':\n            # models are ['KNN','LR',\"RF\",'SVC']\n            models=Categorical('Models',self.Models)\n            #KNN parameters\n            Kneighbors=Integer('Ks',(1,10),default=1)\n            #LR  and svc Parameters\n            rc=Float('regularizationStre',(0.01,1))\n            #RF parameters\n            nestimators=Integer('n_estimators',(1,20),default=10)\n            #SVC parameters\n            kernel=Categorical('kernel',['linear','rbf'])\n            #dependencies\n\n            useks=InCondition(child=Kneighbors,parent=models,values=['KNN'])\n            userc=InCondition(child=rc,parent=models,values=['LR','SVC'])\n            usekernel=InCondition(child=kernel,parent=models,values=['SVC'])\n            useEst=EqualsCondition(child=nestimators,parent=models,value='RF')\n\n\n\n            #adding conditions and HPs\n            confs.add_hyperparameters([models,Kneighbors,rc,nestimators,kernel])\n            confs.add_conditions([useks,userc,usekernel,useEst])\n        elif self.Problemtype=='Regression':\n            models=Categorical('Models',self.Models)\n            #linear regression parameters\n            #lasso and ridge regression parameters\n            alpha=Float('alpha',(0.01,100))\n            \n            #random forest and XGboost parameters\n            nestimators=Integer('n_estimators',(1,20),default=10)\n            #dependencies \n            usealpha=InCondition(child=alpha,parent=models,values=['Lasso','Ridge'])\n            useEst=InCondition(child=nestimators,parent=models,values=['RF','XGboost'])\n            #adding conditions and HPs\n            confs.add_hyperparameters([models,alpha,nestimators])\n            confs.add_conditions([usealpha,useEst])\n        return confs\n    def train(self,config:Configuration,seed: int=0):\n        start_time=time.time()\n        config_dict=config.get_dictionary()\n        model=config_dict['Models']\n        print(f\"config_dict:{config_dict}\")\n        if self.Problemtype=='Classification':\n            return self.classification(config_dict,start_time)\n        elif self.Problemtype=='Regression':\n            return self.regression(config_dict)\n    def classification(self,configDict,start_time):\n        model=configDict['Models']\n        if model=='KNN':\n            Classifier=KNeighborsClassifier(n_neighbors=configDict['Ks'])\n        elif model=='LR':\n            Classifier=LogisticRegression(C=configDict['regularizationStre'])\n        elif model=='RF':\n            Classifier=RandomForestClassifier(n_estimators=configDict['n_estimators'],random_state=42)\n        elif model=='SVC':\n            Classifier=SVC(C=configDict['regularizationStre'],kernel=configDict['kernel'])\n        print(f\"the type of the classifier is: {type(Classifier)}\")\n        Classifier.fit(self.X_train,self.Y_train)\n        y_pred = Classifier.predict(self.X_test)\n        loss=1-accuracy_score(self.Y_test,y_pred)\n        print(\"the loss is: \",loss)\n        return {'loss':loss,'time':time.time()-start_time}\n    def regression(self, configDict,start_time):\n            model=configDict['Models']\n            if model=='LR':\n                regressor=LinearRegression()\n            elif model=='Lasso':\n                regressor=Lasso(alpha=configDict['alpha'])\n            elif model=='Ridge':\n                regressor=Ridge(alpha=configDict['alpha'])\n            elif model=='RF':\n                regressor=RandomForestRegressor(n_estimators=configDict['n_estimators'],random_state=42)\n            elif model=='XGboost':\n                regressor=XGBRegressor(n_estimators=configDict['n_estimators'],random_state=42)\n            regressor.fit(self.X_train,self.Y_train)\n            y_pred = regressor.predict(self.X_test)\n            mse = mean_squared_error(self.Y_test, y_pred)\n            print(\"Mean Squared Error:\", mse)\n            return {'loss':mse,'time':time.time()-start_time}\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from enum import Enum\n\nclass ProblemType(Enum):\n    CLASSIFICATION = 'classification'\n    REGRESSION = 'regression'\n    TIME_SERIES = 'time series'\n    UNBALANCED = 'unbalanced'\n\nclass Facade:\n    def __init__(self, problem_type,Models,X_train,X_test,Y_train,Y_test):\n        if isinstance(problem_type, ProblemType):\n            self.problem_type = problem_type\n            self.models=Models\n            self.X_train=X_train\n            self.X_test=X_test\n            self.Y_train=Y_train\n            self.Y_test=Y_test\n        else:\n            raise ValueError(\"problem_type must be a ProblemType Enum\")\n    def chooseFacade(self):\n        if self.problem_type==ProblemType.CLASSIFICATION:\n            return  self.ClassificationFacade()\n        elif self.problem_type==ProblemType.REGRESSION:\n            return self.RegressionFacade()\n        elif self.problem_type==ProblemType.TIME_SERIES:\n            return self.TimeSeriesFacade()\n        elif self.problem_type==ProblemType.UNBALANCED:\n            return self.UnbalancedFacade()\n    def ClassificationFacade(self):\n        classifier=Models(self.models,'Classification',self.X_train,self.Y_train,self.X_test,self.Y_test)\n        scenario = Scenario(classifier.configspace(), deterministic=True,objectives=['loss','time'], n_trials=100)\n        smac = HyperparameterOptimizationFacade(scenario, classifier.train,overwrite=True,callbacks=[CustomCallback()],\n                                                multi_objective_algorithm=HyperparameterOptimizationFacade.get_multi_objective_algorithm(scenario,objective_weights=[2, 1]))\n        incumbents = smac.optimize()\n        for incumbent in incumbents:\n            print(incumbent)\n        return incumbents\n    def RegressionFacade(self):\n        Regressor=Models(self.models,'Regression',self.X_train,self.Y_train,self.X_test,self.Y_test)\n        scenario = Scenario(Regressor.configspace(), deterministic=True,objectives=['loss','time'], n_trials=100)\n        smac = HyperparameterOptimizationFacade(scenario, Regressor.train,overwrite=True,callbacks=[CustomCallback()],\n                                                multi_objective_algorithm=HyperparameterOptimizationFacade.get_multi_objective_algorithm(scenario,objective_weights=[2, 1]))\n        incumbents = smac.optimize()\n        for incumbent in incumbents:\n            print(incumbent)\n        return incumbents\n    def TimeSeriesFacade(self):\n        pass\n    def UnbalancedFacade(self):\n        pass","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nif __name__ ==\"__main__\":\n#test classifcation\n    classincummbet=Facade(ProblemType.CLASSIFICATION,['KNN','LR','RF','SVC'],X_train_raw,X_test,y_train,y_test)\n    classincummbet.chooseFacade()\n#     print(classincummbet)\n\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import datasets\niris = datasets.load_iris()\n\nX = iris.data[:, 1:]\ny = iris.data[:, 0] \n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if __name__==\"__main__\":\n    regreincumbent=Facade(ProblemType.REGRESSION,['LR','Lasso','Ridge','RF','XGboost'],X_train,y_train,X_test,y_test)\n    regreincumbent.chooseFacade()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor=XGBRegressor(n_estimators=10,random_state=42)\nregressor.fit(X_train,y_train)\ny_pred = regressor.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"regressor=RandomForestRegressor(n_estimators=19)\nregressor.fit(X_train,y_train)\ny_pred = regressor.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Classifier=RandomForestClassifier(n_estimators=19)\nClassifier.fit(X_train_raw,y_train)\ny_pred = Classifier.predict(X_test)\nloss=1-accuracy_score(y_test,y_pred)\nprint(\"the loss is: \",loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}