{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from bestmodel import *\n",
    "from cashAlgorithm.smacClass import ProblemType\n",
    "from cashAlgorithm.Models import SARIMAModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # url = \"https://www.openml.org/data/get_csv/31/dataset_31.csv\"\n",
    "# url='data.csv'\n",
    "# df = pd.read_csv(url)\n",
    "# #export the data to a csv file\n",
    "# # df.to_csv('data.csv', index=False)\n",
    "# df = df.dropna()\n",
    "\n",
    "# # Convert categorical features to numerical\n",
    "# df = pd.get_dummies(df, drop_first=True)\n",
    "# target = \"class_good\"\n",
    "# # X = df.drop(columns=target)\n",
    "# y = df[target]\n",
    "\n",
    "# X_train_raw, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# def Which_features(X_train,y_train,number_of_columns):\n",
    "\n",
    "#     # Select the top k features based on ANOVA F-statistic\n",
    "#     selector = SelectKBest(score_func=f_classif, k=number_of_columns)\n",
    "#     X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "#     # Get the column names of the selected features\n",
    "#     selected_feature_names = X_train.columns[selector.get_support()]\n",
    "\n",
    "    \n",
    "#     return list(selected_feature_names)\n",
    "\n",
    "# selected_features = Which_features(X_train_raw,y_train,number_of_columns=15)\n",
    "# X =X[selected_features]\n",
    "# X_train_raw = X_train_raw[selected_features]\n",
    "# X_test = X_test[selected_features]\n",
    "# # selected_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #try svc with linear kernel and on datasetabove\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# svcobj = SVC(kernel='linear')\n",
    "# svcobj.fit(X_train_raw, y_train)\n",
    "# y_pred = svcobj.predict(X_test)\n",
    "# accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# #testsvm, test regression, test time series\n",
    "# if __name__ == '__main__':\n",
    "#     problemtype = ProblemType.CLASSIFICATION\n",
    "#     choosenModels=['KNN','LR']\n",
    "#     Bestmodelobj = Bestmodel(problemtype,choosenModels,X_train_raw,X_test,y_train,y_test)\n",
    "#     Bestmodelobj.splitTestData()\n",
    "#     Bestmodelobj.TrainModel()\n",
    "#     predictions=Bestmodelobj.PredictModel(X_test)\n",
    "#     print(Bestmodelobj.modelobj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn import datasets\n",
    "# iris = datasets.load_iris()\n",
    "\n",
    "# X = iris.data[:, 1:]\n",
    "# y = iris.data[:, 0] \n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# mse = mean_squared_error(y_test, y_pred)\n",
    "# print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ['LinearRegression','Lasso','Ridge','RF','XGboost']\n",
    "# if __name__ == \"__main__\":\n",
    "#     problemtype = ProblemType.REGRESSION\n",
    "#     choosenModels=['LR','Lasso','Ridge','RF','XGboost']\n",
    "#     Bestmodelobj = Bestmodel(problemtype,choosenModels,X_train,X_test,y_train,y_test)\n",
    "#     Bestmodelobj.splitTestData()\n",
    "#     Bestmodelobj.TrainModel()\n",
    "#     predictions=Bestmodelobj.PredictModel(X_test)\n",
    "#     print(Bestmodelobj.modelobj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from similaritySearch.functions import *\n",
    "from bestmodel import *\n",
    "from cashAlgorithm.smacClass import ProblemType\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def calculate_date_frequency(series):\n",
    "    \"\"\"\n",
    "    Calculate the most common frequency (interval) of a datetime series.\n",
    "\n",
    "    Parameters:\n",
    "    series (pd.Series): A pandas Series of datetime objects.\n",
    "\n",
    "    Returns:\n",
    "    str: A string representing the most common frequency (e.g., 'D' for days, 'H' for hours).\n",
    "    \"\"\"\n",
    "    # Drop NaN values to avoid errors in calculations\n",
    "    series = series.dropna()\n",
    "\n",
    "    # Calculate the differences between consecutive dates\n",
    "    diffs = series.diff().dropna()\n",
    "\n",
    "    # Calculate the most common frequency\n",
    "    freq = diffs.mode()[0]\n",
    "\n",
    "    # Convert the frequency to a string representation\n",
    "    series = series.dropna()\n",
    "\n",
    "    # Calculate the differences between consecutive dates\n",
    "    diffs = series.diff().dropna()\n",
    "\n",
    "    # Calculate the most common frequency\n",
    "    freq = diffs.mode()[0]\n",
    "\n",
    "    # Convert the frequency to a string representation\n",
    "    if freq == pd.Timedelta(days=1):\n",
    "        return 'D'  # Daily\n",
    "    elif freq >= pd.Timedelta(days=7):\n",
    "        return 'W'  # Weekly\n",
    "    elif freq >= pd.Timedelta(days=30):\n",
    "        return 'M'  # Monthly\n",
    "    elif freq >= pd.Timedelta(days=90):\n",
    "        return 'Q'  # Quartely\n",
    "    elif freq >= pd.Timedelta(days=365):\n",
    "        return 'A'  # Annually\n",
    "    else:\n",
    "        return freq  # Return the exact frequency if it's not a common one\n",
    "\n",
    "\n",
    "def Detections_(df, y_column, problem, date_col=None):\n",
    "    if date_col:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    if problem == \"timeseries\":\n",
    "        df.rename(columns={date_col: 'ds', y_column: 'y'}, inplace=True)\n",
    "        y_column = 'y'\n",
    "        df['y'] = df['y'].str.replace('[^0-9\\.]', '', regex=True)\n",
    "        df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "        df['y'] = df['y'].astype(float)\n",
    "\n",
    "    df = RemoveIDColumn.remove_high_cardinality_columns(df)\n",
    "    df = MissingValues().del_high_null_cols(df)\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    nulls_columns = MissingValues().detect_nulls(df)\n",
    "    cols_with_outliers = Outliers().detect_outliers(df)\n",
    "    df = Duplicates().handle_dub(df)\n",
    "    imbalance_detected = None\n",
    "    if problem == \"classification\":\n",
    "        imbalance_detected = HandlingImbalanceClasses().detect_class_imbalance(df, y_column)\n",
    "    df_without_y = df.drop(columns=[y_column])\n",
    "    low_variance_columns, low_variance_info = HandlingColinearity().detect_low_variance(df_without_y)\n",
    "\n",
    "    return df, nulls_columns, cols_with_outliers, imbalance_detected, low_variance_columns, categorical_columns\n",
    "\n",
    "\n",
    "def _process_data(df: pd.DataFrame, fill_na_dict: dict, outliers_methods_input: tuple,\n",
    "                  Norm_method: str) -> pd.DataFrame:\n",
    "    \"\"\"Handle missing values, outliers, normalization, and encoding.\"\"\"\n",
    "    df = MissingValues().handle_nan(df, fill_na_dict)\n",
    "    cols_with_outliers = Outliers().detect_outliers(df)\n",
    "    df = Outliers().handle_outliers(df, cols_with_outliers, outliers_methods_input)\n",
    "    df = DataNormalization().normalize_data(df, Norm_method)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def Cleaning(df, problem, y_column, fill_na_dict, outliers_methods_input, imb_instruction, Norm_method,\n",
    "             lowvariance_actions, encoding_dict, date_col=None):\n",
    "    if problem == \"timeseries\":\n",
    "        date_col = 'ds'\n",
    "        y_column = 'y'\n",
    "        frequency = calculate_date_frequency(df[date_col])\n",
    "        df.set_index('ds', inplace=True)\n",
    "        train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    elif problem == \"classification\":\n",
    "        train_data, test_data = train_test_split(df, test_size=0.2, random_state=42, stratify=df[y_column])\n",
    "        train_data = train_data.reset_index(drop=True)\n",
    "        test_data = test_data.reset_index(drop=True)\n",
    "    else:\n",
    "        df['timestamp'] = df[date_col].astype('int64') / 10 ** 9  # Convert to UNIX timestamp in seconds\n",
    "        df['year'] = df[date_col].dt.year\n",
    "        df['month'] = df[date_col].dt.month\n",
    "        df['day'] = df[date_col].dt.day\n",
    "        df.drop(date_col, axis=1, inplace=True)\n",
    "        train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        train_data = train_data.reset_index(drop=True)\n",
    "        test_data = test_data.reset_index(drop=True)\n",
    "\n",
    "    train_data = _process_data(train_data, fill_na_dict, outliers_methods_input, Norm_method)\n",
    "    test_data = _process_data(test_data, fill_na_dict, outliers_methods_input, Norm_method)\n",
    "\n",
    "    df_copy = pd.concat([train_data, test_data])\n",
    "    df_copy = df_copy.sort_index()\n",
    "    original_columns = set(df.columns)\n",
    "\n",
    "    # Remove co-linearity & low_variance\n",
    "    historical_df_without_y = df_copy.drop(columns=[y_column])\n",
    "    historical_df_without_y = HandlingColinearity().handle_low_variance(historical_df_without_y, lowvariance_actions)\n",
    "    historical_df_without_y = HandlingColinearity().handling_colinearity(historical_df_without_y)\n",
    "    historical_df_without_y[y_column] = df_copy[y_column]\n",
    "\n",
    "    # Update historical_df_copy with processed columns\n",
    "    df_copy = historical_df_without_y.copy()\n",
    "\n",
    "    # Identify removed columns\n",
    "    processed_columns = set(df_copy.columns)\n",
    "    removed_columns = list(original_columns - processed_columns)\n",
    "\n",
    "    # Remove identified columns from train_data and test_data\n",
    "    train_data = train_data.drop(columns=removed_columns)\n",
    "    test_data = test_data.drop(columns=removed_columns)\n",
    "    # if problem != \"timeseries\":\n",
    "    # not needed now\n",
    "    # meta_extractor, meta_features, best_models = extract_and_search_features(df_copy)\n",
    "\n",
    "    train_data = EncodeCategorical().Encode(train_data, encoding_dict)\n",
    "    test_data = EncodeCategorical().Encode(test_data, encoding_dict)\n",
    "\n",
    "    if imb_instruction:\n",
    "        train_data = HandlingImbalanceClasses().handle_class_imbalance(train_data, y_column, imb_instruction)\n",
    "        test_data = HandlingImbalanceClasses().handle_class_imbalance(test_data, y_column, imb_instruction)\n",
    "\n",
    "    df_copy = pd.concat([train_data, test_data])\n",
    "    df_copy = df_copy.sort_index()\n",
    "\n",
    "    if problem != \"timeseries\":\n",
    "        x_train = train_data.drop(columns=[y_column])\n",
    "        y_train = train_data[y_column]\n",
    "\n",
    "        # Split test_data into features and labels\n",
    "        x_test = test_data.drop(columns=[y_column])\n",
    "        y_test = test_data[y_column]\n",
    "\n",
    "        return x_train, y_train, x_test, y_test,\n",
    "    else:\n",
    "        return train_data, test_data, frequency\n",
    "\n",
    "\n",
    "def user_interaction(df, problem, y_column, date_col=None):\n",
    "    try:\n",
    "        df, nulls_columns, cols_with_outliers, imbalance, low_variance_columns, categorical_columns = Detections_(df,\n",
    "                                                                                                                  y_column,\n",
    "                                                                                                                  problem,\n",
    "                                                                                                                  date_col)\n",
    "\n",
    "        # Handling missing values\n",
    "        fill_na_dict = {}\n",
    "        if nulls_columns:\n",
    "            fill_na_dict = {col: 'auto' for col in nulls_columns}\n",
    "\n",
    "        # Handling outliers\n",
    "        outliers_method_input = ('z_score', 'auto', 3)\n",
    "        imb_instruction = \"auto\" if imbalance else None\n",
    "        Norm_method = \"auto\"\n",
    "        low_actions = {}\n",
    "        encoding_dict = {}\n",
    "        if categorical_columns:\n",
    "            encoding_dict = {col: 'auto' for col in categorical_columns}\n",
    "        if low_variance_columns:\n",
    "            low_actions = {col: 'auto' for col in low_variance_columns}\n",
    "\n",
    "        if problem != \"timeseries\":\n",
    "            x_train, y_train, x_test, y_test = Cleaning(df, problem, y_column, fill_na_dict, outliers_method_input,\n",
    "                                                        imb_instruction, Norm_method, low_actions, encoding_dict,\n",
    "                                                        date_col)\n",
    "            return x_train, y_train, x_test, y_test\n",
    "\n",
    "        else:\n",
    "            train_data, test_data, frequency = Cleaning(df, problem, y_column, fill_na_dict, outliers_method_input,\n",
    "                                                        imb_instruction, Norm_method, low_actions, encoding_dict,\n",
    "                                                        date_col)\n",
    "            return train_data, test_data, frequency\n",
    "\n",
    "    except ValueError as ve:\n",
    "        print(f\"Error occurred: {ve}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:95] Reducing the number of initial configurations from 80 to 2 (max_ratio == 0.25).\n",
      "[INFO][abstract_initial_design.py:147] Using 2 initial design configurations and 0 additional configurations.\n",
      "let's start the optimization\n",
      "[INFO][abstract_intensifier.py:305] Using only one seed for deterministic scenario.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[INFO][abstract_intensifier.py:515] Added config 0963f6 as new incumbent because there are no incumbents yet.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "The incumbent is: {'Models': 'Arima', 'd': 2, 'p': 0, 'q': 0}\n",
      "The incumbent loss is: inf\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: inf\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 0\n",
      "Model MSE: 0.9947289190508328\n"
     ]
    }
   ],
   "source": [
    "df1 =pd.read_csv(r\"daily-minimum-temperatures-in-me.csv\")\n",
    "problemtype1 = \"timeseries\"\n",
    "train_data, test_data,frequency = user_interaction(df1, problemtype1, \"Daily minimum temperatures\", date_col=\"Date\")\n",
    "choosenModels=[\"Arima\",'Sarima']\n",
    "traindatax='lol'\n",
    "test_datax='loll'\n",
    "Bestmodelobj = Bestmodel(ProblemType.TIME_SERIES,choosenModels,traindatax,traindatax,train_data,test_data,frequency)\n",
    "Bestmodelobj.splitTestData()\n",
    "# Bestmodelobj.Getincumbent()\n",
    "Bestmodelobj.TrainModel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:95] Reducing the number of initial configurations from 40 to 25 (max_ratio == 0.25).\n",
      "[INFO][abstract_initial_design.py:147] Using 24 initial design configurations and 0 additional configurations.\n",
      "let's start the optimization\n",
      "[INFO][abstract_intensifier.py:305] Using only one seed for deterministic scenario.\n",
      "[INFO][abstract_intensifier.py:515] Added config 61f4fc as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config 7d1035 and rejected config 61f4fc as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config 3f9fd5 and rejected config 7d1035 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[INFO][abstract_intensifier.py:594] Added config d53524 and rejected config 3f9fd5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "The incumbent is: {'Models': 'LR', 'regularizationStre': 0.07983951867558062}\n",
      "The incumbent loss is: 0.202247191011236\n",
      "The incumbent is: {'Models': 'LR', 'regularizationStre': 0.07983951867558062}\n",
      "The incumbent loss is: 0.202247191011236\n",
      "The incumbent is: {'Models': 'LR', 'regularizationStre': 0.07983951867558062}\n",
      "The incumbent loss is: 0.202247191011236\n",
      "The incumbent has remained the same for 30 trials.\n",
      "Stopping the optimization process.\n",
      "[INFO][smbo.py:224] A callback returned False. Abort is requested.\n",
      "[INFO][smbo.py:332] Shutting down because the stop flag was set.\n",
      "Model accuracy: 77.53%\n"
     ]
    }
   ],
   "source": [
    "df2 = pd.read_csv(r\"train.csv\")\n",
    "problemtype2 = \"classification\"\n",
    "choosenModels = [\"KNN\", \"LR\", \"RF\"]\n",
    "x_train, y_train, x_test, y_test = user_interaction(df2, problemtype2, \"Survived\", date_col=None)\n",
    "Bestmodelobj = Bestmodel(ProblemType.CLASSIFICATION, choosenModels, x_train,x_test,y_train,y_test)\n",
    "Bestmodelobj.splitTestData()\n",
    "# Bestmodelobj.Getincumbent()\n",
    "Bestmodelobj.TrainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Temp\\ipykernel_60348\\2759679421.py\", line 3, in <module>\n",
      "    x_train, y_train, x_test, y_test = user_interaction(df3, problemtype3, \"Calories\", date_col=\"Date\")\n",
      "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Temp\\ipykernel_60348\\3285653845.py\", line 162, in user_interaction\n",
      "    df, nulls_columns, cols_with_outliers, imbalance, low_variance_columns, categorical_columns = Detections_(df,\n",
      "                                                                                                  ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Temp\\ipykernel_60348\\3285653845.py\", line 62, in Detections_\n",
      "    df = MissingValues().del_high_null_cols(df)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Alaa\\Desktop\\AutoML\\Auto_ML_Project\\backend\\autoAnalysisServer\\preprocessing_Scripts\\similaritySearch\\functions.py\", line 143, in del_high_null_cols\n",
      "    df[col], status, date_message = self.fill_datetime_na(df[col])\n",
      "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Alaa\\Desktop\\AutoML\\Auto_ML_Project\\backend\\autoAnalysisServer\\preprocessing_Scripts\\similaritySearch\\functions.py\", line 268, in fill_datetime_na\n",
      "    print(\"messageeeeee\",message)\n",
      "                        ^^^^^^^\n",
      "UnboundLocalError: cannot access local variable 'message' where it is not associated with a value\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1428, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1319, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1172, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1087, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 969, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"C:\\Users\\Alaa\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\executing\\executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "df3 = pd.read_csv(r\"pulsedata.csv\")\n",
    "problemtype3 = \"regression\"\n",
    "x_train, y_train, x_test, y_test = user_interaction(df3, problemtype3, \"Calories\", date_col=\"Date\")\n",
    "choosenModels = ['LinearRegression', \"Lasso\"]\n",
    "Bestmodelobj = Bestmodel(ProblemType.REGRESSION, choosenModels, x_train,x_test,y_train,y_test)\n",
    "Bestmodelobj.splitTestData()\n",
    "# Bestmodelobj.Getincumbent()\n",
    "Bestmodelobj.TrainModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"daily-minimum-temperatures-in-me.csv\"\n",
    "df =pd.read_csv(path)\n",
    "df.rename(columns={'Date': 'ds', 'Daily minimum temperatures': 'y'}, inplace=True)\n",
    "df['ds'] = pd.to_datetime(df['ds'], format='%m/%d/%Y')\n",
    "df['y'] = df['y'].str.replace('[^0-9\\.]', '', regex=True)\n",
    "df['y'] = pd.to_numeric(df['y'], errors='coerce')\n",
    "df['y'] = df['y'].astype(float)\n",
    "df.set_index('ds', inplace=True)\n",
    "split_date = pd.to_datetime('1990-12-15')\n",
    "train_data = df[df.index <= split_date]\n",
    "test_data = df[df.index > split_date]\n",
    "traindatax='lol'\n",
    "test_datax='loll'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ds</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1981-01-01</th>\n",
       "      <td>20.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-02</th>\n",
       "      <td>17.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-03</th>\n",
       "      <td>18.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-04</th>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981-01-05</th>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-12-11</th>\n",
       "      <td>11.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-12-12</th>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-12-13</th>\n",
       "      <td>11.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-12-14</th>\n",
       "      <td>12.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-12-15</th>\n",
       "      <td>13.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3634 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               y\n",
       "ds              \n",
       "1981-01-01  20.7\n",
       "1981-01-02  17.9\n",
       "1981-01-03  18.8\n",
       "1981-01-04  14.6\n",
       "1981-01-05  15.8\n",
       "...          ...\n",
       "1990-12-11  11.1\n",
       "1990-12-12  14.0\n",
       "1990-12-13  11.4\n",
       "1990-12-14  12.5\n",
       "1990-12-15  13.4\n",
       "\n",
       "[3634 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Models import SARIMAModel\n",
    "# sarimasmac(train, test, p, q, d, P, Q, D, s, freq='D')\n",
    "# results=SARIMAModel.Sarimasmac(train_data, test_data, 2, 0, 0, 3, 1, 3, 30, freq='D')\n",
    "# resul/ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO][abstract_initial_design.py:95] Reducing the number of initial configurations from 80 to 2 (max_ratio == 0.25).\n",
      "[INFO][abstract_initial_design.py:147] Using 2 initial design configurations and 0 additional configurations.\n",
      "let's start the optimization\n",
      "[INFO][abstract_intensifier.py:305] Using only one seed for deterministic scenario.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[INFO][abstract_intensifier.py:515] Added config 31b9e5 as new incumbent because there are no incumbents yet.\n",
      "[INFO][abstract_intensifier.py:594] Added config 04a5f9 and rejected config 31b9e5 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[INFO][abstract_intensifier.py:594] Added config e96d2b and rejected config 04a5f9 as incumbent because it is not better than the incumbents on 1 instances:\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "[WARNING][abstract_runner.py:134] Target function returned infinity or nothing at all. Result is treated as CRASHED and cost is set to inf.\n",
      "The incumbent is: {'Models': 'Sarima', 'd': 1, 'p': 0, 'q': 2, 'sd': 0, 'sp': 0, 'sq': 1, 'ss': 30}\n",
      "The incumbent loss is: 23.06167266189422\n",
      "[INFO][smbo.py:327] Configuration budget is exhausted:\n",
      "[INFO][smbo.py:328] --- Remaining wallclock time: inf\n",
      "[INFO][smbo.py:329] --- Remaining cpu time: inf\n",
      "[INFO][smbo.py:330] --- Remaining trials: 0\n",
      "Model MSE: 12.614792084692727\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    problemtype = ProblemType.TIME_SERIES\n",
    "    choosenModels=['Sarima']\n",
    "    Bestmodelobj = Bestmodel(problemtype,choosenModels,traindatax,traindatax,train_data,test_data)\n",
    "    Bestmodelobj.splitTestData()\n",
    "    # Bestmodelobj.Getincumbent()\n",
    "    Bestmodelobj.TrainModel()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=Bestmodelobj.PredictModel(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1990-12-16    13.043275\n",
       "1990-12-17     9.459163\n",
       "1990-12-18    11.359163\n",
       "1990-12-19    10.559163\n",
       "1990-12-20     8.959163\n",
       "1990-12-21    15.659163\n",
       "1990-12-22    12.859163\n",
       "1990-12-23    11.159163\n",
       "1990-12-24    11.759163\n",
       "1990-12-25    13.659163\n",
       "1990-12-26    13.559163\n",
       "1990-12-27     9.459163\n",
       "1990-12-28    15.359163\n",
       "1990-12-29    15.359163\n",
       "1990-12-30    13.359163\n",
       "1990-12-31    13.959163\n",
       "Freq: D, Name: predicted_mean, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
